{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_dir = \"../data\"\n",
    "train_df = pd.read_csv(f\"{input_dir}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = [\n",
    "    # infinite execution\n",
    "    '25F9B9BAA02A',\n",
    "    '49586CD6A649',\n",
    "    '6F896BABB13C',\n",
    "    'BECA14914CFB',\n",
    "    'E7A3DBC919C1',\n",
    "    'FC9BC150809F',\n",
    "    'FFC43F453EF6',\n",
    "    'EFCA46E0BF9F',\n",
    "\n",
    "    # spelling errors\n",
    "    'C30B2AD4AF0A',\n",
    "    '718800CC3C50',\n",
    "    '9C2E6F09CC73',\n",
    "\n",
    "    # nan in argumentation rankings\n",
    "    '8D4A0D4CD2C2',\n",
    "    '129497C3E0FC']\n",
    "\n",
    "train_df = train_df.loc[~train_df.essay_id.isin(exclude_list)].copy().reset_index(drop=True)\n",
    "\n",
    "train_df[\"discourse_elements_number\"] = train_df.groupby(train_df.essay_id).discourse_id.transform('count')\n",
    "train_df = train_df.loc[train_df.discourse_elements_number < 15].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=['ner', 'parser', 'lemmatizer', 'textcat'])\n",
    "parser = English()\n",
    "\n",
    "def punctuation_removal(tokens):\n",
    "    punctuations = string.punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuations]\n",
    "    return tokens\n",
    "\n",
    "def stopwords_removal(tokens):\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stems = [porter.stem(token) for token in tokens]\n",
    "    return stems\n",
    "    \n",
    "def lemmatization(tokens):\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "    \n",
    "def spacy_tokenizer(phrase, steps=[punctuation_removal]):\n",
    "    phrase = phrase.strip().lower()\n",
    "    tokens = parser(phrase)\n",
    "    tokens = [token.text for token in tokens]\n",
    "    \n",
    "    for step in steps:\n",
    "        tokens = step(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('rankings.txt', 'r') as f:\n",
    "    rankings = f.readlines()\n",
    "\n",
    "rankings = [ranking[1:-2].split('],[') for ranking in rankings]\n",
    "for ranking in rankings:\n",
    "    ranking[0] = ranking[0][1:]\n",
    "    ranking[-1] = ranking[-1][:-1]\n",
    "rankings = [ranking for ranking_list in rankings for ranking in ranking_list]\n",
    "rankings = {ranking.split(',')[0]: ranking.split(',')[1] for ranking in rankings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['bwaf_rank'] = train_df.discourse_id.map(rankings.__getitem__)\n",
    "train_df['bwaf_rank'] = train_df['bwaf_rank'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful modules\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(train_df.discourse_type.values.reshape(-1, 1))\n",
    "print(enc.categories_)\n",
    "train_df = pd.concat([train_df, pd.DataFrame(enc.transform(train_df.discourse_type.values.reshape(-1, 1)).toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_validation(pipe, X, y, n_splits=10):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    log_losses = []\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        pipe.fit(X.iloc[train_index], y[train_index])\n",
    "        predicted = pipe.predict_proba(X.iloc[val_index])\n",
    "        current_log_loss = metrics.log_loss(y[val_index], predicted)\n",
    "        # current_accuracy = metrics.accuracy_score(y[val_index], predicted.argmax(axis=1))\n",
    "\n",
    "        log_losses.append(current_log_loss)\n",
    "    return log_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "classifier = AdaBoostClassifier()\n",
    "\n",
    "bwaf_rank_transformer = FunctionTransformer(lambda x: x[['bwaf_rank', 0, 1, 2, 3, 4, 5, 6]])\n",
    "discourse_transformer = FunctionTransformer(lambda x: x.discourse_text.map(clean_text))\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_feature', bwaf_rank_transformer),\n",
    "        ('text_features', Pipeline([\n",
    "            ('selector', discourse_transformer),\n",
    "            ('vectorizer', vectorizer)]))])),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "X = train_df.loc[:, ['discourse_text', 'bwaf_rank', 0, 1, 2, 3, 4, 5, 6]]\n",
    "\n",
    "l_enc = LabelEncoder()\n",
    "l_enc.fit(train_df.discourse_effectiveness.values)\n",
    "y = l_enc.transform(train_df.discourse_effectiveness.values)\n",
    "\n",
    "log_loss = 0\n",
    "log_losses_with_rank = []\n",
    "for i in range(0, 10):\n",
    "    current_log_losses = cross_validation(pipe, X, y)\n",
    "    log_losses_with_rank.extend(current_log_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "classifier = AdaBoostClassifier()\n",
    "discourse_transformer = FunctionTransformer(lambda x: x.discourse_text.map(clean_text))\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('selector', discourse_transformer),\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "X = train_df.loc[:, ['discourse_text']]\n",
    "l_enc = LabelEncoder()\n",
    "l_enc.fit(train_df.discourse_effectiveness)\n",
    "y = l_enc.transform(train_df.discourse_effectiveness)\n",
    "\n",
    "log_loss = 0\n",
    "log_losses_without_rank = []\n",
    "for i in range(0, 10):\n",
    "    current_log_losses = cross_validation(pipe, X, y)\n",
    "\n",
    "    print(f'Log losses at run {i}: {current_log_losses}')\n",
    "    log_losses_without_rank.extend(current_log_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the difference between the results\n",
    "diff = [y - x for y, x in zip(log_losses_with_rank, log_losses_without_rank)]\n",
    "\n",
    "#Comopute the mean of differences\n",
    "d_bar = np.mean(diff)\n",
    "\n",
    "#compute the variance of differences\n",
    "sigma2 = np.var(diff)\n",
    "\n",
    "# in a 10-fold cross validation the size of the test sample is 1 tenth of the original size\n",
    "n1 = 0.9\n",
    "n2 = 0.1\n",
    "\n",
    "# 10-fold cross validation repeated 10 times\n",
    "k = 100\n",
    "\n",
    "# compute the modified variance\n",
    "sigma2_mod = sigma2 * (1/k + n2/n1)\n",
    "# compute the t_static\n",
    "t_static =  d_bar / np.sqrt(sigma2_mod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('feedback-prize')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9d741e4017939f24011eef60972f408e10152fbc2f78d1e2aa08f0b9ddfb14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

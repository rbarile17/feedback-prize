{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 831 # max number of words in a discourse text\n",
    "n_epochs = 10 # how many times to iterate over all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "exclude_list = [\n",
    "    # infinite execution\n",
    "    '25F9B9BAA02A',\n",
    "    '49586CD6A649',\n",
    "    '6F896BABB13C',\n",
    "    'BECA14914CFB',\n",
    "    'E7A3DBC919C1',\n",
    "    'FC9BC150809F',\n",
    "    'FFC43F453EF6',\n",
    "    'EFCA46E0BF9F',\n",
    "\n",
    "    # spelling errors\n",
    "    'C30B2AD4AF0A',\n",
    "    '718800CC3C50',\n",
    "    '9C2E6F09CC73',\n",
    "\n",
    "    # nan in argumentation rankings\n",
    "    '8D4A0D4CD2C2',\n",
    "    '129497C3E0FC']\n",
    "\n",
    "train_df = train_df.loc[~train_df.essay_id.isin(exclude_list)].copy().reset_index(drop=True)\n",
    "\n",
    "train_df[\"discourse_elements_number\"] = train_df.groupby(train_df.essay_id).discourse_id.transform('count')\n",
    "train_df = train_df.loc[train_df.discourse_elements_number < 15].reset_index(drop=True).copy()\n",
    "\n",
    "with open('rankings.txt', 'r') as f:\n",
    "    rankings = f.readlines()\n",
    "\n",
    "rankings = [ranking[1:-2].split('],[') for ranking in rankings]\n",
    "for ranking in rankings:\n",
    "    ranking[0] = ranking[0][1:]\n",
    "    ranking[-1] = ranking[-1][:-1]\n",
    "rankings = [ranking for ranking_list in rankings for ranking in ranking_list]\n",
    "rankings = {ranking.split(',')[0]: ranking.split(',')[1] for ranking in rankings}\n",
    "\n",
    "train_df['bwaf_rank'] = train_df.discourse_id.map(rankings.__getitem__)\n",
    "train_df['bwaf_rank'] = train_df['bwaf_rank'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = train_df[\"discourse_text\"]\n",
    "features = train_df[\"bwaf_rank\"]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad the sentences \n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "features = np.array(features)\n",
    "\n",
    "# Get the target values\n",
    "enc = LabelEncoder()\n",
    "enc.fit(train_df.discourse_effectiveness.values)\n",
    "y = enc.transform(train_df.discourse_effectiveness.values)\n",
    "\n",
    "# Shuffling the data\n",
    "trn_idx = np.random.permutation(len(X))\n",
    "\n",
    "X = X[trn_idx]\n",
    "features = features[trn_idx]\n",
    "y = y[trn_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "model_w2v = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def initialize_embeddings(embeddings, tokenizer):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.index_word) + 1, embedding_size), dtype=np.float32)\n",
    "                                \n",
    "    for idx, word in enumerate(tokenizer.index_word): \n",
    "        if word in embeddings:\n",
    "            embedding_matrix[idx+1,:] = embeddings[word]\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = initialize_embeddings(model_w2v, tokenizer)\n",
    "embedding_matrix = torch.from_numpy(embedding_matrix).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=8, feats=False):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.feats = feats\n",
    "\n",
    "        # MLP for numerical features\n",
    "        self.input_size = input_size\n",
    "        self.mlp_hidden_size  = 64\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.mlp_hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        # biLSTM for text\n",
    "        self.hidden_size = 256\n",
    "        # self.embedding = nn.Embedding(max_features, embedding_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.lstm = nn.LSTM(embedding_size, self.hidden_size, bidirectional=True, num_layers=2)\n",
    "        if feats:\n",
    "            self.linear = nn.Linear((4 * self.hidden_size) + self.input_size, 3)\n",
    "        else:\n",
    "            self.linear = nn.Linear(4 * self.hidden_size, 64)\n",
    "\n",
    "        drp = 0.3\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(64, 3)\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, numeric_feats=None):\n",
    "        embeddings = self.embedding(x)\n",
    "        lstm_output, _ = self.lstm(embeddings)\n",
    "\n",
    "        avg_pool = torch.mean(lstm_output, 1)\n",
    "        max_pool, _ = torch.max(lstm_output, 1)\n",
    "        conc = torch.cat((avg_pool, max_pool), 1)\n",
    "\n",
    "        if self.feats:\n",
    "            # mlp_output = self.fc1(bwaf_rank)\n",
    "            conc = torch.cat((conc, numeric_feats), 1)\n",
    "        \n",
    "        lstm_output = self.relu(self.linear(conc))\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "\n",
    "        logits = self.out(lstm_output)\n",
    "        \n",
    "        log_logits = self.log_softmax(logits)\n",
    "        logits = self.softmax(logits)\n",
    "\n",
    "        return log_logits, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        data,target = self.dataset[index]\n",
    "        return data,target,index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "oh_enc.fit(train_df.discourse_type.values.reshape(-1, 1))\n",
    "discourse_type = oh_enc.transform(train_df.discourse_type.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "features = np.hstack([features.reshape(-1, 1), discourse_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "def do_train(epoch, train, train_loader, features_train, model, loss_fn, optimizer, pbar):\n",
    "    running_loss = 0.\n",
    "    for iteration, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "        # Predict/Forward Pass\n",
    "        feats = torch.tensor(features_train[index]).unsqueeze(dim=1).cuda()\n",
    "        y_pred_log, y_pred = model(x_batch, feats.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred_log, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        this_loss = loss.item()\n",
    "        running_loss += this_loss\n",
    "        pbar.set_postfix(epoch=epoch, loss = running_loss / (iteration + 1))\n",
    "        pbar.update()\n",
    "\n",
    "def do_eval(epoch, val, val_loader, features_val, model, loss_fn, optimizer):\n",
    "    val_preds = np.zeros((len(val), len(enc.classes_)))\n",
    "    val_loss = 0.\n",
    "    for iteration, (x_batch, y_batch, index) in enumerate(val_loader):\n",
    "        # Predict/Forward Pass\n",
    "        feats = torch.tensor(features_val[index]).unsqueeze(dim=1).cuda()\n",
    "        y_pred_log, y_pred = model(x_batch, feats.float())\n",
    "        val_preds[index] = y_pred.detach().cpu().numpy()\n",
    "\n",
    "    return val_preds\n",
    "\n",
    "def do_fold( X, y, features):\n",
    "    params = [\n",
    "        (32, 0.0005, False), (32, 0.005, False), (64, 0.0005, False), (64, 0.005, False), (128, 0.0005, False), (128, 0.005, False),\n",
    "        (32, 0.0005, True), (32, 0.005, True), (64, 0.0005, True), (64, 0.005, True), (128, 0.0005, True), (128, 0.005, True)]\n",
    "\n",
    "    log_losses = []\n",
    "    best_epochs = []\n",
    "    for param in params:\n",
    "        batch_size = param[0]\n",
    "        lr = param[1]\n",
    "        use_feats = param[2]\n",
    "\n",
    "        train_idx, val_idx, _, _ = train_test_split(range(0, X.shape[0]), y, stratify=y, test_size=0.2) \n",
    "\n",
    "        X_train = torch.tensor(X[train_idx], dtype=torch.long).cuda()\n",
    "        y_train = torch.tensor(y[train_idx], dtype=torch.long).cuda()\n",
    "\n",
    "        X_val = torch.tensor(X[val_idx], dtype=torch.long).cuda()\n",
    "        y_val = torch.tensor(y[val_idx], dtype=torch.long).cuda()\n",
    "\n",
    "        features_train = features[train_idx]\n",
    "        features_val = features[val_idx]\n",
    "\n",
    "        train = MyDataset(torch.utils.data.TensorDataset(X_train, y_train))\n",
    "        val = MyDataset(torch.utils.data.TensorDataset(X_val, y_val))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = BiLSTM(use_feats)\n",
    "        model.cuda()\n",
    "\n",
    "        loss_fn = nn.NLLLoss()\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        best_model = model\n",
    "        best_log_loss = np.inf\n",
    "        best_epoch = -1\n",
    "        patience = 0\n",
    "        with tqdm(desc=f'Train params ({batch_size}, {lr}, {use_feats})', unit='iteration', total=len(train_loader) * n_epochs) as pbar:\n",
    "            for epoch in range(0, n_epochs):\n",
    "                do_train(epoch, train, train_loader, features_train, model, loss_fn, optimizer, pbar)\n",
    "                val_preds = do_eval(epoch, val, val_loader, features_val, model, loss_fn, optimizer)\n",
    "\n",
    "                log_loss = metrics.log_loss(y_val.cpu().numpy(), val_preds)\n",
    "                if log_loss < best_log_loss:\n",
    "                    best_log_loss = log_loss\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_epoch = epoch\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                \n",
    "                if patience == 2:\n",
    "                    break\n",
    "\n",
    "        val_preds = np.zeros((len(val), len(enc.classes_)))\n",
    "        with tqdm(desc=f'Final evaluation', unit='iteration', total=len(val_loader)) as pbar:\n",
    "            for iteration, (x_batch, y_batch, index) in enumerate(val_loader):\n",
    "                # Predict/Forward Pass\n",
    "                feats = torch.tensor(features_val[index]).unsqueeze(dim=1).cuda()\n",
    "                y_pred_log, y_pred = best_model(x_batch, feats.float())\n",
    "                pbar.update()\n",
    "                val_preds[index] = y_pred.detach().cpu().numpy()\n",
    "        \n",
    "        log_losses.append(metrics.log_loss(y_val.cpu().numpy(), val_preds))\n",
    "        best_epochs.append(best_epoch)\n",
    "\n",
    "    # print(log_losses)\n",
    "    # print(best_epochs)\n",
    "\n",
    "    min_idx = log_losses.index(min(log_losses))\n",
    "    return params[min_idx], best_epochs[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True).split(X, y))\n",
    "log_losses = []\n",
    "params = []\n",
    "best_epochs = []\n",
    "for i, (train_idx, test_idx) in enumerate(splits):\n",
    "    print(f'Fold {i}')\n",
    "    X_train = X[train_idx.astype(int)]\n",
    "    y_train = y[train_idx.astype(int)]\n",
    "\n",
    "    X_test = X[test_idx.astype(int)]\n",
    "    y_test = y[test_idx.astype(int)]\n",
    "\n",
    "    features_train = features[train_idx.astype(int)]\n",
    "    features_test = features[test_idx.astype(int)]\n",
    "\n",
    "    param, best_epoch = do_fold(X_train, y_train, features_train)\n",
    "    params.append(param)\n",
    "    best_epochs.append(best_epoch)\n",
    "\n",
    "    batch_size = param[0]\n",
    "    lr = param[1]\n",
    "    use_feats = param[2]\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.long).cuda()\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).cuda()\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.long).cuda()\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).cuda()\n",
    "\n",
    "    train = MyDataset(torch.utils.data.TensorDataset(X_train, y_train))\n",
    "    test = MyDataset(torch.utils.data.TensorDataset(X_test, y_test))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = BiLSTM(use_feats)\n",
    "    model.cuda()\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    with tqdm(desc=f'Train with best params: ({batch_size}, {lr}, {use_feats}, {best_epoch})', unit='iteration', total=len(train_loader) * (best_epoch + 1)) as pbar:\n",
    "        for epoch in range(0, best_epoch + 1):\n",
    "            do_train(epoch, train, train_loader, features_train, model, loss_fn, optimizer, pbar)\n",
    "    test_preds = do_eval(epoch, test, test_loader, features_test, model, loss_fn, optimizer)\n",
    "\n",
    "    log_loss = metrics.log_loss(y_test.cpu().numpy(), test_preds)\n",
    "    print(f'Log loss at fold {i}: {log_loss}')\n",
    "    log_losses.append(log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "print(best_epochs)\n",
    "print(log_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('feedback-prize')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9d741e4017939f24011eef60972f408e10152fbc2f78d1e2aa08f0b9ddfb14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
